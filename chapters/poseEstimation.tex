%pose estimation
\chapter{Pose Estimation}
\label{poseEstimation}

In this chapter we discuss our approach to detecting and tracking the pose of an articulated object through a series of images. We begin with a discussion of our approach and demonstrate its performance in a number of different test cases. We conclude with a discussion of prior work in the fields of object recognition, segmentation, and pose estimation. 

\section{Background}

Augmented and virtual reality, which we refer to in this thesis as \textit{mixed reality}, are progressing at a rapid pace. This advancement is due largely to improvements in hardware and in our understanding of the human visual system. Collectively, these advancements mean that mixed reality devices are becoming ergonomic and are capable of delivering content that seamlessly blends with the surrounding world. In particular, this progress means that it is now reasonable to develop mixed reality applications for users interacting in dynamic, unpredictable environments. Consequently, there are two situations for which pose tracking is required. The first is to track the user as she navigates an environment in order to deliver the current pose of the device, necessary for delivering content. The second situation involves tracking both people and objects that occupy the environment surrounding the user. Many mixed reality platform providers (references?) address the first problem (of finding the position of the headset) by tracking the internal dynamics of the headset with embedded inertial sensors. For better performance and for more complex behaviours many providers also require that the user operates within a controlled environment that features a number of cameras around its perimeter. These cameras track the user as they move around the environment, which allows for more interactivity between the user and the application, and improves the positional tracking system within the application, which improves the display performance. In most cases, however, this visual tracking system relies on distinctive visual markers placed on the user. In addition, few systems include capabilities to track external actors. Applications would become far more immersive if mixed reality platforms were capable of tracking arbitrary objects in their environment. 

\subsection{Terminology}

In this thesis we refer to an \textit{actor} as any object, whether it is a person, a car, or a [endemic NZ bird]. We differentiate between the cases where the actor is \textit{rigid} or \textit{non-rigid}, and whether they are \textit{articulated} or not. A human hand, for instance, is a non-rigid articulated object, due to the fact that the fingers can move independently (typically under certain constraints). In contrast, a solid object such as a desk is not articulated (if one ignores any drawers or movable appendages). A human hand is non-rigid because the skin undergoes complex non-linear motions in response to an underlying rigid motion. In this work however we ignore these non-linear effects and treat the hand as a rigid body object. Similarly we treat a walking person as a rigid-body, articulated actor. In the previous chapter we saw that a bounding box could be used to define a template for an actor and to specify its location in the image. This presents a number of difficulties for tracking articulated actors. An articulated actor occupies only a small percentage of the space within a bounding box. By definition, therefore, a filter that tracks an articulated actor will either fail to correctly identify the target, or the classifier will naturally drift as more and more background features are included in the representation. To mitigate these effects we need to use a more natural representation of articulated actors. We therefore use the following definition:

\theoremstyle{definition}
\begin{definition}
\label{articulatedActor}
Given an object represented as a graph over a collection of nodes, the object is rigid and non-articulated if the Euclidean distances between the nodes are fixed. A rigid object is articulated if the Euclidean distances between nodes are free to change, while the Geodesic distances on the mesh remain constant. 
\end{definition}

[actor -> graph -> model]

Actors are represented as a graph $G = (V, E)$. We have that $V = \{v_i\}$ is a set of nodes, $E = \{e_{ij}\}$ is a set of edges and $e_{ij} > 0$ denotes an edge between node $i$ and node $j$. Our graph is weighted, such that that weight on each edge represents the Euclidean distance between it's two nodes, i.e. $e_{ij} = d(i,j)$. The geodesic distances between any two nodes is the sum of the edges that lie on the shortest path between them. Intuitively this works in the same way as a skeleton. At any point in time separate joints that are not connected through a bone may be any aribitrary Euclidean distance from each other. If we were to find the shortest path between the ends of each of our thumbs, however, we would find that the distance we travel along the bones remains constant at any point in time. Given a series of 2D measurements taken from a moving articulated object, Ross (2010) use a probabilistic formulation to reconstruct the most likely graph from the data.

[probably a good place for a diagram]

In order to reduce the complexity of the problem we assume that any actor we wish to track can be specified by a graph $G$ ahead of time. This means that the distances between the edges are fixed, which  This dramatically reduces our search space, as many of the nodes in our graph are now constrained to move in a particular fashion. 

Given a model of the actor, represented as a graph, how do we represent the pose? There are a number of alternative measures for this, such as exponential maps (), quaternions, and Euler angles. We choose instead to use rotation and translation matrices as these are convenient and we are only dealing with small graphs, so we are not operating under memory constraints. 

[Go into more detail about how the model is put together and how the rotation matrices work etc. + homogeneous coordinates.]

We wish to find the current pose of the actor in every frame of the video. We are choosing in this work to rely on top-down approaches, whereby we place a model into the scene and record the degree to which it fits the image. We then adjust the pose of the model in such a way that after a certain number of iterations we can be confident the pose of the model will align with the true underlying actor. 

We begin by specifying the true location of the actor in the first frame, and from this build our initial model of the actor. Evaluating the model at any given pose means sampling, for every point in the model, the image intensity at that pixel, the distance to the nearest edge and the optical flow information at that point. The similarity between any current pose of our model and the set of positive (or negative) examples is given by the Hilbert-Schmidt independence criterion, as outlined in section x. Using the HSIC as a measure of the similarity between two random variables is advantageous as it captures all of the functional dependencies between the two samples. There are of course many other measures we could use for the similarity, such as the normalised cross correlation (NCC) and the mutual information. The NCC is a linear measure, however, and therefore fails to capture non-linear effects such as dynamic changes in lighting. This is significant, as it is not often the case that positive changes in one variable linearly correlate with positive changes in another variable (or negative changes). Changes tend to be subtle and difficult to discern. In theory the mutual information is the most robust measure for capturing and understanding these non-linear dependencies. The mutual information requires, however, that the underlying probability distributions of the sample from the random variable be estimated. This can be both computationally expensive and can lead to errors in the approximation. Notably, to represent a probability distribution with a histogram, we typically need the number of samples to be much greater than both the dimensionality of the features we are sampling and the size of their respective domains. We can refer to this problem as \textit{coverage} (curse of dimensionality?) which refers to the fact that we need enough samples to cover the domain of every dimension, otherwise the probability distribution can be unnecessarily concentrated in certain regions.



[1. curse of dimensionality diagram?]
[2. example of sampling procedure (intensities, edges etc)]
[3. would be good to show example of the energy surface]
[4. do a comparison of different optimisation procedures]


.\\
.\\
.\\
.\\


object recognition chapter\\
what is pose estimation?\\
	- rigid vs non-rigid\\
	- articulations\\


The most effective mechanism for estimating the pose of an articulated actor is to use fiducial markers. These small markers, placed on specific locations on the body, serve as easy points to track in a multi-camera setup. The points can be easily differentiated from the background and therefore the trajectories of the joints can be reasonably reconstructed given a video. In every frame of the video the positions of the markers can be estimated by applying blob detection to the images from each of the cameras. Smoothness constraints on the motion of the actor allow for correspondences between blobs in consecutive frames to be established, providing the camera frame rate is high enough. The only problems that typically arise in marker based tracking systems are due to occlusions, however this can be easily resolved by increasing the number of cameras available. While marker based tracking systems are fast and reliable, they are often impractical due to the high cost, the need for multiple calibrated cameras, and because they require physical markers be attached to the actor prior to performance capture. Markerless motion capture systems, most of which rely on computer vision principles, are much more extensively studied and allow for a greater range of applications. The principle focus of this thesis is on the use of markerless motion capture systems for pose estimation. 

Why is it that we can't use a bounding box representaion to track articulated objects? Imagine, for instance, that we are using an object localisation framework, such as TLD or STRUCK, and are using only a template of the object learnt in the first frame. Two major problems arise in this paradigm. The first is that our object will naturally occupy only a portion of the bounding box. Tracking a person with their arms outstretched, for instance, requires a bounding box that naturally includes a large background region in the area under the arms. This background region is included as part of the foreground region and therefore affects tracking performance. Such a tracker would almost immediately fail, even if the actor kept her arms in the same relative position throughout the sequence, as the classification process would need to match the initial background (that was included as part of the foreground) with new background regions. We therefore need a tracking framework that allows for regions other than rectangular bounding boxes to be tracked. 

The most common framework for pose estimation of articulated objects is known as \textit{generative tracking}, or alternatively as model-based tracking or analysis-by-synthesis. Generative tracking attempts to minimise a model-to-image cost function for every frame in the video. The three main components of any generative tracking framework are the choice of model, the cost function, and the optimisation procedure. Popular generative tracking schemes include PoseCut, which jointly segments an image and optimises the pose of the actor in each frame using conditional random fields (). Similarly, (vineet and sheasby) jointly estimate the optimal segmentation, pose and depth of an articulated actor using a conditional random field. Oikonomidis (hand occlusions) demonstrate a generative technique that matches hand poses to edge and colour maps extracted from an image, while simultaneously preventing any two rigid bodies from intersecting. This method therefore solves for the optimal pose in the presence of physical objects that restrict the space of poses of the hand. 

Pose estimation frameworks generally fall into two categories: generative and discriminative, which can also be called top-down and bottom-up respectively. 









































