%pose estimation
\chapter{Pose Estimation}
\label{poseEstimation}

In this chapter we discuss our approach to detecting and tracking the pose of an articulated object through a series of images. We begin with a discussion of our approach and demonstrate its performance in a number of different test cases. We conclude with a discussion of prior work in the fields of object recognition, segmentation, and pose estimation. 

\section{Background}

Augmented and virtual reality, which we refer to in this thesis as \textit{mixed reality}, are progressing at a rapid pace. This advancement is due largely to improvements in hardware and in our understanding of the human visual system. Collectively, these advancements mean that mixed reality devices are becoming ergonomic and are capable of delivering content that seamlessly blends with the surrounding world. In particular, this progress means that it is now reasonable to develop mixed reality applications for users interacting in dynamic, unpredictable environments. Consequently, there are two situations for which pose tracking is required. The first is to track the user as she navigates an environment in order to deliver the current pose of the device, necessary for delivering content. The second situation involves tracking both people and objects that occupy the environment surrounding the user. Many mixed reality platform providers (references?) address the first problem (of finding the position of the headset) by tracking the internal dynamics of the headset with embedded inertial sensors. For better performance and for more complex behaviours many providers also require that the user operates within a controlled environment that features a number of cameras around its perimeter. These cameras track the user as they move around the environment, which allows for more interactivity between the user and the application, and improves the positional tracking system within the application, which improves the display performance. In most cases, however, this visual tracking system relies on distinctive visual markers placed on the user. In addition, few systems include capabilities to track external actors. Applications would become far more immersive if mixed reality platforms were capable of tracking arbitrary objects in their environment. 

\subsection{Terminology}

In this thesis we refer to an \textit{actor} as any object, whether it is a person, a car, or a [endemic NZ bird]. We differentiate between the cases where the actor is \textit{rigid} or \textit{non-rigid}, and whether they are \textit{articulated} or not. A human hand, for instance, is a non-rigid articulated object, due to the fact that the fingers can move independently (typically under certain constraints). In contrast, a solid object such as a desk is not articulated (if one ignores any drawers or movable appendages). A human hand is non-rigid because the skin undergoes complex non-linear motions in response to an underlying rigid motion. In this work however we ignore these non-linear effects and treat the hand as a rigid body object. Similarly we treat a walking person as a rigid-body, articulated actor. In the previous chapter we saw that a bounding box could be used to define a template for an actor and to specify its location in the image. This presents a number of difficulties for tracking articulated actors. An articulated actor occupies only a small percentage of the space within a bounding box. By definition, therefore, a filter that tracks an articulated actor will either fail to correctly identify the target, or the classifier will naturally drift as more and more background features are included in the representation. To mitigate these effects we need to use a more natural representation of articulated actors. We therefore use the following definition:

\theoremstyle{definition}
\begin{definition}
\label{articulatedActor}
Given an object represented as a graph over a collection of nodes, the object is rigid and non-articulated if the Euclidean distances between the nodes are fixed. A rigid object is articulated if the Euclidean distances between nodes are free to change, while the Geodesic distances on the mesh remain constant. 
\end{definition}

[actor -> graph -> model]

Actors are represented as a graph $G = (V, E)$. We have that $V = \{v_i\}$ is a set of nodes, $E = \{e_{ij}\}$ is a set of edges and $e_{ij} > 0$ denotes an edge between node $i$ and node $j$. Our graph is weighted, such that that weight on each edge represents the Euclidean distance between it's two nodes, i.e. $e_{ij} = d(i,j)$. The geodesic distances between any two nodes is the sum of the edges that lie on the shortest path between them. Intuitively this works in the same way as a skeleton. At any point in time separate joints that are not connected through a bone may be any aribitrary Euclidean distance from each other. If we were to find the shortest path between the ends of each of our thumbs, however, we would find that the distance we travel along the bones remains constant at any point in time. Given a series of 2D measurements taken from a moving articulated object, Ross (2010) use a probabilistic formulation to reconstruct the most likely graph from the data.

[probably a good place for a diagram]

In order to reduce the complexity of the problem we assume that any actor we wish to track can be specified by a graph $G$ ahead of time. This means that the distances between the edges are fixed, which  This dramatically reduces our search space, as many of the nodes in our graph are now constrained to move in a particular fashion. 

Given a model of the actor, represented as a graph, how do we represent the pose? There are a number of alternative measures for this, such as exponential maps (), quaternions, and Euler angles. We choose instead to use rotation and translation matrices as these are convenient and we are only dealing with small graphs, so we are not operating under memory constraints. 

[Go into more detail about how the model is put together and how the rotation matrices work etc. + homogeneous coordinates.]

We wish to find the current pose of the actor in every frame of the video. We are choosing in this work to rely on top-down approaches, whereby we place a model into the scene and record the degree to which it fits the image. We then adjust the pose of the model in such a way that after a certain number of iterations we can be confident the pose of the model will align with the true underlying actor. 

We begin by specifying the true location of the actor in the first frame, and from this build our initial model of the actor. Evaluating the model at any given pose means sampling, for every point in the model, the image intensity at that pixel, the distance to the nearest edge and the optical flow information at that point. The similarity between any current pose of our model and the set of positive (or negative) examples is given by the Hilbert-Schmidt independence criterion, as outlined in section x. Using the HSIC as a measure of the similarity between two random variables is advantageous as it captures all of the functional dependencies between the two samples. There are of course many other measures we could use for the similarity, such as the normalised cross correlation (NCC) and the mutual information. The NCC is a linear measure, however, and therefore fails to capture non-linear effects such as dynamic changes in lighting. This is significant, as it is not often the case that positive changes in one variable linearly correlate with positive changes in another variable (or negative changes). Changes tend to be subtle and difficult to discern. In theory the mutual information is the most robust measure for capturing and understanding these non-linear dependencies. The mutual information requires, however, that the underlying probability distributions of the sample from the random variable be estimated. This can be both computationally expensive and can lead to errors in the approximation. Notably, to represent a probability distribution with a histogram, we typically need the number of samples to be much greater than both the dimensionality of the features we are sampling and the size of their respective domains. We can refer to this problem as \textit{coverage} (curse of dimensionality?) which refers to the fact that we need enough samples to cover the domain of every dimension, otherwise the probability distribution can be unnecessarily concentrated in certain regions.



[1. curse of dimensionality diagram?]
[2. example of sampling procedure (intensities, edges etc)]
[3. would be good to show example of the energy surface]
[4. do a comparison of different optimisation procedures]


.\\
.\\
.\\
.\\


object recognition chapter\\
what is pose estimation?\\
	- rigid vs non-rigid\\
	- articulations\\


\subsection{Marker-based Tracking}
The most effective mechanism for estimating the pose of an articulated actor is to use fiducial markers. These small markers, placed on specific locations on the body, serve as easy points to track in a multi-camera setup. The points can be easily differentiated from the background and therefore the trajectories of the joints can be reasonably reconstructed given a video. In every frame of the video the positions of the markers can be estimated by applying blob detection to the images from each of the cameras. Smoothness constraints on the motion of the actor allow for correspondences between blobs in consecutive frames to be established, providing the camera frame rate is high enough. The only problems that typically arise in marker based tracking systems are due to occlusions, however this can be easily resolved by increasing the number of cameras available. While marker based tracking systems are fast and reliable, they are often impractical due to the high cost, the need for multiple calibrated cameras, and because they require physical markers be attached to the actor prior to performance capture. Markerless motion capture systems, most of which rely on computer vision principles, are much more extensively studied and allow for a greater range of applications. The principle focus of this thesis is on the use of markerless motion capture systems for pose estimation. 

Why is it that we can't use a bounding box representaion to track articulated objects? Imagine, for instance, that we are using an object localisation framework, such as TLD or STRUCK, and are using only a template of the object learnt in the first frame. Two major problems arise in this paradigm. The first is that our object will naturally occupy only a portion of the bounding box. Tracking a person with their arms outstretched, for instance, requires a bounding box that naturally includes a large background region in the area under the arms. This background region is included as part of the foreground region and therefore affects tracking performance. Such a tracker would almost immediately fail, even if the actor kept her arms in the same relative position throughout the sequence, as the classification process would need to match the initial background (that was included as part of the foreground) with new background regions. We therefore need a tracking framework that allows for regions other than rectangular bounding boxes to be tracked. 


\subsection{Generative Tracking}

The most common framework for pose estimation of articulated objects is known as \textit{generative tracking}, or alternatively as model-based tracking or analysis-by-synthesis (surveys - moeslund, erol, poppe, sigal). Generative tracking attempts to minimise a model-to-image cost function for every frame in the video. The three main components of any generative tracking framework are the choice of model, the cost function, and the optimisation procedure. Popular generative tracking schemes include PoseCut, which jointly segments an image and optimises the pose of the actor in each frame using conditional random fields (). Similarly, (vineet and sheasby) jointly estimate the optimal segmentation, pose and depth of an articulated actor using a conditional random field. Oikonomidis (hand occlusions) demonstrate a generative technique that matches hand poses to edge and colour maps extracted from an image, while simultaneously preventing any two rigid bodies from intersecting. This method therefore solves for the optimal pose in the presence of physical objects that restrict the space of poses of the hand. Elhayek () present an elegant solution for finding the optimal skeletal and camera parameters by introducing a Sum of Gaussians body model (). By jointly solving for both the camera and pose parameters Elhayek are able to track moving targets from multiple moving, and therefore uncalibrated, cameras. This is significant as camera calibration is a costly process that generally restricts multi-camera pose estimation to cases where the relative position between the cameras is fixed, for instance in stereo reconstruction methods. In contrast, methods for monocular target tracking are prevalent, for example Andriluka (monocular2010) which combines a discriminative 2D people detector and a parts based generative tracking framework to track multiple articulated objects with a single camera. Parts based methods are becoming increasingly common as they separate the recognition problem into multiple subcomponents, each of which are theorised to be easier to solve. Kwon (2013), for instance, treat non-rigid objects as a collection of patches. The pose is represented by the topology between patches and an online model is used to learn appearance classifiers for each of the patches over time. Kiefel \& Gehler (2014) use a conditional markov random field to model the local appearance and the spatial configuration of consituent body parts. These studies are an example of bottom-up, or \textit{discriminative} techniques for pose estimation. They are exemplified by a learning stage that constructs a database or model of training examples. 

Pose estimation techniques can be classified as either \textit{generative}, \textit{discriminative}, or a mixture of the two. Discriminative techniques closely resemble object recognition approaches discussed in the previous chapter. They are characterised by an initial training phase, from which a model is learnt that can be used to output likely 3D poses from images. Conversely, generative techniques optimise the pose of a model that best fits an image. Discriminative techniques are therefore sometimes known as \textit{bottom-up} while generative techniques are known as \textit{top-down}. Discriminative techniques are powerful because they can incoporate a great deal of prior knowledge about shape (Agarwal triggs), appearance (Grabner 2006, andrilukar, kiefel gehler) and motion (akhter) of the target. This becomes limiting, however, as offline learning can be costly, it can be difficult to acquire training databases and the algorithm can be prone to overfitting. This is especially problematic for pose estimation frameworks due to the typically high number of degrees of freedom of the actor, for example. Similarly, articulated objects can undergo signifiant appearance changes and often become self-occluded. In contrast, generative techniques require only a simple model of the target and a function that measures the cost of projecting a given pose of the model into the image. Generative techniques can therefore be adapted to a much wider range of problems and have a greater degree of flexibility in dealing with new and previously unseen situations. They obviously suffer, however, from local minima, especially in regions of high clutter, and can be inefficient in monocular camera systems. 


\subsection{Discriminative Tracking}

Perhaps the most representative example of discriminative tracking comes from (2003athitsosestimating), who use a database of synthetic hand images to estimate the 3D pose of a hand in an image. Their database is constructed by generating an extensive range of synthetic hand images, in a number of different poses for each camera viewpoint. Given any input image the distance to the nearest database image is computed from the Chamfer distance between edge maps in the respective images. A similar view based representation was developed by (1998blackeigentracking) who construct a low-dimensional set of basis images by finding the eigenvectors of a matrix constructed from training images. Tracking is then formulated as a least-squares matching problem. Agarwal and Triggs automatically extract shape descriptor vectors from specific pose silhouettes against which they perform regression, using known motion capture joint angles as the desired output. Pose estimation is performed in subsequent frames by passing the sampled shape descriptors through the regression framework to uncover the 3D joint angles. A number of authors have also proposed techniques that estimate temporal trajectories of an actor by matching to known motion databases. In particular, (2015akhter3dpose) develop a prior model of pose dependent joint limits from a motion capture dataset. They then use this prior to help disambiguate between the multiple ambiguous 3D poses that arise from 2D point observations. (2014zhouspatiotemporal) adopt a similar approach, whereby they learn offline a kinematic model from motion capture data which they then fit to sampled trajectories in every frame. (2001rosalesestimating) introduce a technique that allows the 3D pose to be estimated from a number of uncalibrated cameras. They learn a mapping from image features to 2D joint configurations in a number of virtual cameras. The joint 3D structure and camera poses are then estimated from the set of 2D pose hypotheses. They formulate the reconstruction problem as a probabilistic formulation, for which they use expectation maximisation to find a solution.

So far the techniques we have described have largely focussed on a whole-body formulation for tracking. This is suitable in many instances, however tracking performance often suffers when the target is partially occluded as there may not be enough evidence to support detection. In order to address this problem a number of authors have introduced parts-based classification and tracking schemes. (2012zhangrobust) extract a number of small image patches that best describe a target. The patches are chosen to maximise the entropy and to be the most spatially discriminative. This joint formulation allows for the system to select stable patches that span the target and are maximally discriminative. Notably (2012zhangrobust) only demonstrated their technique using rectangular bounding boxes, even though they demonstrated its ability to track articulated objects. We believe this method could be extended to track the pose of the targets by enforcing spatial constraints between the patches.  Similarly, (2013kwonhighly) introduce a parts-based method to track articulated objects, however they too do not explicitly estimate the pose of the target. Their method uses a number of small rectangular patches to represent the target and uses a Bayesian formulation to maximise photometric and geometric likelihoods of the collection of patches in each frame. This method is capable of tracking highly articulated and fast moving targets that exhibit a significant amount of motion blur across frames. In order to reliably track the object in such challenging conditions the authors use an extension of the Metropolis-Hastings algorithm () that effectively searchs large regions of the sample space while searching for minima. The astute reader will note that these techniques are more appropriately designated as non-rigid / articulated extensions of the object recognition paradigm introduced earlier. We include them here, however, as we believe they are much more expressive than their single bounding box counterparts, as they naturally induce a representation that is free to undergo complex articulated motions. (2011wangsuperpixel) learn an appearance model that can discriminate pixels as belonging to either the foreground or background. They improve the robustness of their approach by modelling each pixel as an element of a superpixel, and optimising over pixel assignments to superpixels and the labelling of each superpixel. A similar approach by (2012godechoughbased) uses generalised hough transforms to represent an object. The pixel based representation of the target is learnt in the first frame and the hough transform used to find the most likely set of pixels to represent the target in subsequent frames. This approach allows the target to undergo significant deformations and changes in viewpoint, while maintaining stable tracking. 
 
\subsection{Gait Recognition}

An important element of many surveillance systems and gaming applications is the ability to recognise the gait of a walking person. Many discriminative techniques have been proposed for this problem. Typical approaches track the motion trajectories of the moving object and then classify the motion according to some recognition framework. (2003wangautomatic), for example, use background subtraction to extract the silhouettes of a walking figure in every frame of a video. They then use Procrustes analysis () to extract the mean shape of a particular subject, followed by a further Procrustes analysis to recognise the gait given a training database. 

\subsection {Articulated Structure from Motion}
There also exist a subset of discriminative tracking techniques that learn a model configuration from 2D tracking data. This problem is often known as structure from motion. Taycher (2009) learn the topology of an articulated structure by observing the rigid motion of body segments. They use a probabilistic formulation from which they can recover a tree-structure through factorisation. Ross (2008) extend Taycher by learning both the association of tracked points to body segments and the connections between segments. Structure from motion for a single moving rigid object can be obtained by a factorisation of the matrix of feature point trajectories using singular value decomposition (Tomasi Kanade 2002). Costeira and Kanade 1996 extend this solution to deal with multiple objects by introducing a greedy algorithm that groups feature trajectories before factorisation. Unfortunately, however, Yan and Pollefeys (2005a) have shown that the motion of two groups of points is linearly dependent if the two groups are physically connected. This is due to the fact that the motion subspaces of each of the groups intersect which therefore prevents the feature trajectories from being grouped. Yan and Pollefeys introduce an approach that first segments the feature point trajectories into a collection of rigid parts and then constructs an affinity matrix that describes the likelihood that any two parts are connected by a joint. They then solve for the location of the joints by constructing a spanning tree that connects parts with maximum affinity. The approach of Ross (2008) is similar to Yan and Pollefeys. They use a graphical model to assign trajectories to parts and to represent connections between segments and then iterate over different assignments, using expectation maximisation to find the optimal assignments of part and joint labels.  


































































































