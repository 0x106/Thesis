\chapter{Mathematical Preliminaries}
\label{mathbasics}


The prinicple contribution of this thesis is to demonstate the utility of probability and information theory measures for addressing the correspondence problem in certain computer vision applications. In particular we will examine how the \textit{mutual information} (MI) and the \textit{Hilbert Schmidt independence criterion} (HSIC) can be used to evaluate the relationship between two objects. We begin with an introduction to key concepts in probability theory, then demonstrate how these concepts are central to the development of information theory. We conclude with a discussion of the mutual information. In the following chapter we introduce the reader to Hilbert spaces, then demonstrate how Hilbert spaces relate to earlier concepts in probability and information theory. 

\section{Elements of Probability Theory}

\subsection{Probability spaces}

A probability space is a triple $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a sample, or outcome space, $\mathcal{F}$ is the event space, and $P : \mathcal{F} \rightarrow \mathbb{R}$ is a function that maps events to probabilities. The sample space refers to the set of all outcomes that could occur, while the element $A \in \mathcal{F}$ is a subset of the event space such that $A_i \subseteq \Omega$. The function $P : \mathcal{F} \rightarrow \mathbb{R}$ assigns a value to elements of the event space that describe the likelihood of that event occuring. The probability function must satisfy the following properties (2013grayentropy):

\begin{itemize}
	\item \textit{nonnegative:} 
		\begin{equation}
			P(A) \geq 0, \:\: \forall A \in \mathcal{F};
		\end{equation}
	\item \textit{normalised:} 
		\begin{equation}
			P(\Omega) = 1;
		\end{equation}
	\item \textit{countably additive:} 
		$$\text{If} \: A_i \in \mathcal{F}, i = 1, 2, ... \text{are disjoint, then}$$
		\begin{equation}
			P(\bigcup_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty}{P(A_i)}.
		\end{equation}
\end{itemize}

The first condition states that probability values must be greater than or equal to zero, while the second and third conditions collectively state that 

In the context of computer vision, for example, the sample space is defined by the application. For a set of images the sample space will change depending on factors such as the width and height of the image, the number of channels used to describe each pixel and the range of values each channel can take. The sample space for the set of 8-bit gray-scale images with a fixed width and height is given by the set of all possible configurations of pixel intensities. 

\subsection{Random variables \& Distributions}

(2013durretprobability)
Given our probability space we define a random variable $\mathcal{X}$ to be a function that maps the sample space to some measure space, often the space of real numbers, i.e. $\mathcal{X}\: : \: \Omega \rightarrow \mathbb{R}$. The random variable $\mathcal{X}$ is then said to induce a distribution on $\mathbb{R}$ by letting the probability that $X = a$ be the measure of the set $\{\omega \in \Omega : \mathcal{X}(\omega) = a\}$, which is denoted by $P(\mathcal{X} = a)$. Intuitively, a random variable is an entity that can take on the value of any element of the sample space with probability given by $P$. Drawing a single sample from a distribution is then akin to selecting a particular value for the random variable, according to the probability space. In this work we will denote by $P(x)$ the probability that $\mathcal{X}$ takes on the particular value $x$, i.e. that $P(\mathcal{X} = x)$. Given two random variables, $\mathcal{X}$, and $\mathcal{Y}$, we can define the joint probability $P(\mathcal{X},\mathcal{Y})$ to be the probability distribution for the set of $N$ ordered pairs $\{(x_i,y_i) : x_i \in \mathcal{X}, y_i \in \mathcal{Y}\}$.

We may choose our random variable to be a gray scale image, for example. In the absence of any domain information it may be the case that every pixel value is equally likely to occur, and therefore we describe our random variable as having a uniform distribution. In contrast, we may assume that our image is drawn from a set of images of the natural world, so that it may contain a plant or an animal. In this case there is some restriction on the particular pixel values that are likely to occur, and we therefore say that our random variable is governed by some unknown probability distribution function that we may wish to know more about. We may be interested in knowing, for instance, whether our image contains a kiwi (reference). In our sample space from which we draw images of the natural world there is a probability function that describes the particular configurations of pixels that give rise to images that appear to be kiwi's. Given an image, we may wish to know whether the distribution that generated the current image is equivalent to the true distribution that generates images of kiwi's. This example is obviously far from trivial, as one cannot simply prescribe a function for describing any and all flightless birds from New Zealand. Describing probability functions and their relationship to each other has been the subject of an extraordinary amount of work. In computer science we typically call this problem pattern recognition, which reflects the fact that our goal is to detect patterns and make inferences on the state of the world. 

\subsection{Moments}

In order to understand probability distribution functions we need a language to describe them. This is achieved by computing the \textit{moments} of the distribution. For a discrete univariate probability density function, with an expected value $\mu_1$, the $n^{th}$ moment of the distribution is given by

\begin{equation}
\mu_n = \sum_{i=1}{(x_i - \mu_1)^{n}P(x_i)}.
\end{equation}

\noindent The expectation value, or first moment, of the distribution is given by

\begin{equation}
\mu_1 = \sum_{i=1}{x_i P(x_i)},
\end{equation}

\noindent which is the sum of the individual elements, weighted by their respective probabilities. For a sample of size $N$ the arithmetic mean is given by 

\begin{equation}
\bar{x} = \frac{1}{N}\sum_{i=1}{x_i}.
\end{equation}

\noindent The second moment of the distribution, is given by

\begin{equation}
\mu_2 = \sum_{i=1}{(x_i - \mu_1)^{2}P(x_i)},
\end{equation}

\noindent which is the expected value of the squared deviation from the mean. For a discrete sample the second moment is defined as the variance and is given by 

\begin{equation}
\sigma = \frac{1}{N-1}\sum_{i=1}{(x_i - \bar{x})}.
\end{equation}

\section{Information Theory}

Since Claude Shannon's seminal 1948 paper "A Mathematical Theory of Communication", the field of information theory has developed rapidly. [Give some examples etc].

%$X = \{x_i\}$ and $Y = \{y_i\}$ from the random variables $\mathcal{X}$ and $\mathcal{Y}$

 Much of the early work concerning information theory was intended to be used to develop methods for telephony, and in particular to understand theoretical limits for which a message could be transmitted across a noisy channel. Shannon sought a function that would describe the uncertainty associated with a particular random variable. Shannon recognised that the \textit{entropy}, previously found in statistical mechanics as Boltmann's entropy equation (reference), was a suitable function for relating the probability distribution of a random variable to its uncertainty (\cite{shannon2001mathematical}). For a sample $X = \{x_i\}$ from a random variable $\mathcal{X}$ with corresponding probability distribution $P = \{p_1, p_2, ...., p_N\}$ the Shannon entropy is 

\begin{equation}
	H(X) = \sum_i^N{p_i \: \text{log} \: (p_i)}.
\end{equation}

\noindent Further, \cite{rrnyi1961measures} demonstrated that the Shannon entropy is the limiting case as $\alpha \rightarrow 1$ in a continuous family of functions that satisfy

\begin{equation}
	H_\alpha(X) = \frac{1}{1 -\alpha} \: \text{log}\:\sum_i^N{{p_i}^\alpha}.
\end{equation}

\noindent The Renyi entropy is defined for $\alpha = 2$ as 

\begin{equation}
	\label {renyientropy}
	H_2(X) = - \text{log} \sum_i^N{{p_i}^2}.
\end{equation}

\noindent The joint entropy of the random variables $X$, and $Y$, is given by

\begin{equation}
	H(X, Y) = \sum_i^N\sum_k^N{p_{i,k} \: \text{log} \: (p_{i,k})},
\end{equation}

\noindent where $p_{i,k} = P(x_i,y_k)$ and $Y = \{y_i\}$ is a sample from the random variable $\mathcal{Y}$. One of the key characteristics of the entropy is that it is minimised for events that are either very likely or very unlikely to occur. As a consequence, the entropy is maximised when we are most uncertain about the outcome of an event. Given a Bernoulli trial (such as flipping a coin) the entropy is maximised when the probability of either outcome is equal, as can be seen in (bernoulli entropy figure). 

%\begin{figure}
%	include '~/Desktop/Thesis/Thesis/figureCode/bernoulliEntropy.png'
%\end{figure}

%Note that when $X$ and $Y$ are independent, then we have that $p_{i,k} = p_ip_k$, i.e. that the joint probability of $x_i$ and $y_k$ is equal to the product of their respective probabilities. This means that at independence $H(i,k) = $ 

One of the principle measures to utilise the entropy is the mutual information (reference (shannon?)), defined by

\begin{equation}
	I(X, Y) = H(X) + H(Y)  - H(X, Y).0
\end{equation}

The mutual information is the reduction in uncertainty in $X$, given observations of $Y$ (and vice versa). Notably, the mutual information can be used to characterise the dependency between $X$ and $Y$. We can express the mutual information in terms of probabilities as 


\begin{equation}
	I(X, Y) = \sum_i^N\sum_k^N{p_{i,k} \: \text{log} \: (\frac{p_{i,k}}{p_ip_k})},
\end{equation}

\noindent which is zero if and only if $\mathcal{X}$ and $\mathcal{Y}$ are independent, i.e. if and only if $p_{i,k} = p_i\:p_k$. 

We can see that the mutual information will be greater when the respective entropies are maximised and when the joint entropy is maximised. Consider, for example, the outcome of two Bernoulli trials, such as flipping two coins $\text{T}_1$ and $\text{T}_2$ in succession. In order for the mutual information to be maximised we wish to reduce our uncertainty about the outcome of $\text{T}_2$ after observing $\text{T}_1$ by as much as possible. This occurs when our initial uncertainty, i.e our uncertainty about the outcome of $\text{T}_1$, is maximised and our uncertainty about $\text{T}_2$ given $\text{T}_1$ is minimised. We can see that this occurs, for example, when $P(\text{T}_1 = \textit{heads}) = 0.5$ and $P(\text{T}_1 = \textit{heads}, \text{T}_2 = \textit{heads}) = 1$ (with all other joint probabilities equal to zero).

In order to compute the mutual information we need a mechanism to compute the probabilities $p_i$. It is common to use either histograms or Parzen window techniques to approximate the underlying densities. Given samples $X = \{x_i\}$ and $Y = \{y_i\}$, the histogram is constructed by tallying the frequency of occurance of each particular value. We then compute the entropy as per \ref{renyientropy}.

We compute the Parzen density estimate of the mutual information according to the framework laid out by \cite{wells1996multi}. The Parzen window technique (\cite{duda2012pattern}) aproximates a probability distribution as a superposition of functions centered on each of the elements of a sample. Any differentiable function that integrates to one can be used in Parzen density estimation. In this work, as in \cite{wells1996multi}, we will use the Gaussian function:

\begin{equation}
	G_\psi(x) = \frac{1}{\sqrt[]{2\pi|\psi|}} \text{exp}\left(\frac{1}{2}x^\text{T}\psi^{-1}x\right),
\end{equation}

\noindent where $\psi$ is an empirically chosen covariance matrix. In order to approximate the entropy we create two new samples $A$ and $B$ from $X$, with $N_A, N_B < N$ their respective sizes. The entropy is approximated according to 

\begin{equation}       
	H(X) \approx H^*(X) = \frac{-1}{N_B} \sum_{x_i \in B}{\log \frac{1}{N_A} \sum_{x_j \in A}{G_\psi(x_i - x_j)}},
\end{equation}

whereby an individual probability is approximated according to

\begin{equation}
	p(x) \approx p^*(x) = \frac{1}{N_A} \sum_{x_j \in A}{G_\psi(x - x_j)}.
\end{equation}

In his paper, \cite{shannon2001mathematical} states that "The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point".


\section{Functional Analysis}

The principle theoretical framework we are using in this thesis is that of reproducing kernel Hilbert spaces (RKHS). Specifically, we are interested in the Hilbert-Schmidt independence criterion (HSIC), which measures the cross-covariance between two RKHS's associated with two random variables. We begin this section with an introduction to Hilbert spaces and to the more general theory of a function space. We then introduce the reproducing property and demonstrate its utility in evaluating functions from a Hilbert space. Finally, we discuss how the theory of RKHS motivates the use of the HSIC to measure independence.  

\subsection{Hilbert spaces}

Most readers will be familiar with the concept of the space of real numbers, $\mathcal{R}$, which is an example of a vector space. We can characterise this space as a set of dimensions, whereby points in the space take on a single real value from each of the dimensions and for which operations such as addition and scalar multiplication are permitted. Furthermore, we can endow points in the space with certain properties, such as a norm, which typically represents the size, or magnitude of the vector from the origin to the point. Interestingly, we can also represent \textit{functions} as a vector space. Specifically, a function space $\mathcal{F}$ is defined as a collection of functions which supports the following definitions:

\theoremstyle{definition}
\begin{definition}
(from gdurret03)
An inner product is a function $\langle \cdot, \cdot\rangle : \mathcal{F} \times \mathcal{F} \rightarrow \mathcal{R}$ that satisfies, for every $f, g \in \mathcal{F}$ and $\alpha \in \mathcal{R}$:
\begin{itemize}
\item {Symmetric: $\langle f, g\rangle = \langle g, f\rangle$}
\item {Linear: $\langle f, g\rangle = \langle g, f\rangle$}
\item {Symmetric: $\langle f, g\rangle = \langle g, f\rangle$}
\end{itemize}
\end{definition}

\theoremstyle{definition}
\begin{definition}
(from gdurret03)
A norm is a non-negative function $\langle \cdot, \cdot\rangle : \mathcal{F} \times \mathcal{F} \rightarrow \mathcal{R}$ that satisfies, for every $f, g \in \mathcal{F}$ and $\alpha \in \mathcal{R}$:
\begin{itemize}
\item {Symmetric: $\langle f, g\rangle = \langle g, f\rangle$}
\item {Symmetric: $\langle f, g\rangle = \langle g, f\rangle$}
\item {Symmetric: $\langle f, g\rangle = \langle g, f\rangle$}
\end{itemize}
\end{definition}.

Note that there exist a rich class of both inner products and norms that characterise many different types of spaces. The Euclidean norm, $||x|| = \sqrt{\sum{x_i}^2}$, and the Euclidean dot product $\langle x, x\rangle = xx^T$ are simple examples. It is important to note that dealing with the abstract notion of a norm or inner product between functions is no more trivial in theory than dealing with the norm or inner product between vectors of real numbers (although it may well be in practice). 

In this work we consider a function space known as a Hilbert space. 








































