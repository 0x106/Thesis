%pose estimation
\chapter{Statistical Measures of Dependence}
\label{hsic}

In this chapter we describe the mathematical foundations on which the Hilbert-Schmidt independence criterion is prescribed. For a further comprehensive review the reader may be interested in (Muandet et al 2016). 


\section{Kernels}

The goal of almost every machine learning problem is to discover a \textit{funtion} which relates input data to output data. In the simplest case, the data is given in the form 

\begin{equation}
(x_1, y_1), ..., (x_m, y_m) \in \mathbb{X} \times \mathbb{R}^N
\end{equation}

\noindent where $\{x_i\}$ are input vectors and $\{y_i\}$ are (typically) output scalars. In order to learn a function that reliably assigns output labels to input values we must deal with two (related) problems: (1) how do we compute the similarity between inputs $x_j$ and $x_k$, and (2) do the inputs permit a boundary that can be found by our choice of algorithm? In the simplest case we could use the Euclidean dot product as a similarity measure on the inputs and assign labels to test values according to the training inputs they lie closest to. This amounts to finding a hyperplane that separates the input cases, and is easily visualised as a line separating each class of input in two-dimensions. Unfortunately, however, this example requires that the input data is \textit{linearly separable}, which is never guaranteed. \\

% talk about how the outputs determine whether it's classification or regression.

One approach to the linear separability problem is to use neural networks, which are often effective at learning a non-linear decision boundary that separates classes. Whilst both popular, and effective, neural networks will not be the focus of this work. Rather, we will explore the field of \textit{reproducing kernel Hilbert spaces} (RKHS), which allows us to perform what is colloquially known as the \textit{kernel trick}. Rather than operating in the original input space we project each of the input examples to a higher (and possibly infinite) dimensional space where we perform inference. Obviously, however, it is likely that operating in this high dimensional space is intractable, therefore we choose the space in such a way that we can compute inner products without explicitly computing the high dimensional features. Formally, we wish to find a map

\begin{equation}
\Phi \colon \mathbb{X} \rightarrow \mathbb{H},
\end{equation} 

\noindent and a function that allows

\begin{equation}
k(x_i, x_j) = \langle \Phi(x_i), \Phi(x_j) \rangle
\end{equation} 

\noindent such that $\Phi(x)$ is a mapping of the point $x$ to a feature space and $k(x, x)$ is a function that computes dot products between features. Given the inputs $x_1, ..., x_m \in \mathbb{X}$ we refer to $k \colon \mathbb{X}^2 \rightarrow \mathbb{R}$ as a \textit{kernel} and to $K_{ij} := k(x_i, x_j)$ as its associated $m \times m$ \textit{Gram matrix}.

 %Note that although in this instance we are using input data from a vector field, this does not have to be the case. We may have, for instance, that our data is a graph, a string, or a collection. In simple cases our function may be manually defined, such as $y_i = {x_i}^2$. Conversely, we may use a neural network to learn the function parameters, or we may use an algorithm such as support vector machines. In all these cases the goal is to minimise a {loss function}, defined as the degree to which our learned function can generate output values for a given input. The loss function is minimised when the algorithm is accurate and correctly predicts the output value for each input. Examples of loss functions include

% In order to make predictions using the data we need some way of measuring the similarity between the inputs. This is typically achieved using an inner product between input data, i.e.

% \begin {equation}
% |x_i - x_j| = \langle x_i, x_j \rangle. 
% \end {equation}

% In simple circumstances we may simply take the inner product to be the Euclidean dot product between input features, i.e. given $x_i = \{ x_{i1}, x_{i2} \}$ and $x_j = \{ x_{j1}, x_{j2} \}$, the dot product is given by $ \angle x_i, x_j \rangle = [] []^T$. Note, however that this is often too simple a measure for most circumstances, and should typically be replaced with more robust measures. We therefore need to develop a more general measure, that will allow us to represent 

% \section {Reproducing Kernel Hilbert Spaces}

% Given that we have a means of projecting data into a high dimensional feature space, can we guarantee that there will be a dot product that will allow us to compare input features? To do this we need to understand certain properties of kernels, and in particular a notion called \textit{universality}. Note also injective kernels. 

% --> make some notes about Mercer Kernels
% --> also need to talk about the Reisz representation theory ( and what about dual spaces??)

% This is all really important because if you can use an inner product then you can replace it with a kernel measure which allows for the introduction of highly non-linear features. This is notably important when we wish to differentiate between distribtions that lie in the plane and are non-separable. By projecting into higher dimensional space one often finds that they become separable - known as the kernel trick. This has become immensely popular, especially in such applications as kernel SVM and kernel PCA. 

% \section{Functional Analysis}

% We begin our discusion by exploring the necessary components for describing Hilbert spaces, and, by extension, reproducing kernel Hilbert spaces. In order to describe Hilbert spaces we will see that we need to have an intuitive understanding of a function space and a norm on that space. This will then allow us to describe how we can embed probability distributions in reproducing kernel Hilbert spaces and subsequently compute the statistical dependence between various distributions.\\

% As children, one of the most important ideas we are taught is that numbers have a magnitude. We know that two is less than four, while $380$ is greater than $-19$. Similarly, we are taught that the distance between two numbers can be computed by subtracting one from the other, which allows us to determine the degree to which two numbers differ from each other. As we grow older we are taught that we can construct vectors from these same numbers and apply the same concepts of magnitude and distance. Two vectors $x_1 = (2, 4)$ and $x_2 = (8, 6)$ have magnitudes $|x_1| = \sqrt{5}$ and $|x_2| = \sqrt{100}$ respectively, while the distance between them is given by the magnitude of the vector $z = (-6, -2)$, i.e $|z| = \sqrt{40}$. In both these examples we use the term \textit{norm} to describe the quantity that relates an object, such as a vector or a scalar, to its size. \\

% In these examples we are dealing with real numbers in Euclidean space, i.e. $\textbf{x} \in \mathbb{R}$. However it turns out that one can (non-trivially) extend these concepts to deal with \textit{functions} in a \textit{function space}. We will describe here only those classes $X$ of functions $f \in X$ that include a norm (i.e. \textit{normed function spaces}). The norm is itself a function that assigns a non-negative number $||f||_X$ to every function $f \in X$. Importantly, any norm associated with a function space describes elements of the space in a particular manner.  

% \section{Kernel Embeddings}

% % Many algorithms, such as ~~, choose instead to use a nonlinear function, $\phi$,  which maps points in $X$ to a high-dimensional feature space $F$, i.e. via:

% % \begin{equation}
% % \phi \colon X \rightarrow F \\
% % \end{equation} 

% % \noindent which implies that $x \rightarrow \phi(x)$, where $F$ is a possibly infinite dimensional space. How though, do we choose $\phi(x)$? Imagine we are given the input vector, $x = (x_1, x_2)$. We could in theory select $\phi(x) = \langle x, x \rangle = (x_1^2 + x_2^2)$, however we saw earlier that this is not an expressive relationship. \\

% Most pattern recognition problems begin with the empirical data:

% \begin{equation}
% (x_1, y_1), ..., (x_m, y_m) \in X \times \mathbb{R},
% \end{equation} 

% \noindent where inputs $x_i$ are taken from the domain $X$ and targets $y_i$ from $\mathbb{R}$. The goal of pattern recognition is to build a classifier that accurately predicts the labels given input values. This naturally requires that we have a measure of the similarity between inputs $x_i$ and $x_k$, the formulation of which is the focus of the following section. \\

% In order to measure the similarity between various inputs it could be the case that we choose, for instance, to use the inner product $\langle x, y \rangle$, however this is a linear relationship and not expressive enough for most applications. It turns out, however, that we can extend the concept of the inner product to a general class known as \textit{product features}, which rely on the concept of \textit{monomials}. Formally, we wish to find a non-linear mapping of our domain to a high-dimensional feature space, within which we can compute an inner product. We therefore wish to find

% \begin{equation}
% \phi \colon X \rightarrow F 
% \end{equation} 

% \noindent where taking the inner product $\langle \dot, \dot \rangle$ in $F$ is equivalent to evaluating the \textit{kernel function} for two input points, i.e.

% \begin{equation}
% k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle
% \end{equation} 

% \noindent where $x \rightarrow \phi(x)$ is a mapping from $x$ to its high-dimensional counterpart. As noted, the inner product relies on the computation of monomials, which are products of powers of two variables with non-negative exponents. Monomials are commonly encountered as the individual terms in polynomials and are specified by their degree $d$, which is the sum of the exponents of each of the variables in the monomial. As an example, the monomial $z_1{z_2}^2$ has degree three, while the monomial ${z_1}^2{z_2}^3{z_3}^4$ has degree nine. It is often the case that we wish to compute the set of monomials of a given degree. For a set $Z = \{z_1, z_2\}$ the monomials of degree three are $\{{z_1}^3, {z_2}^3, {z_1}^2{z_2}, {z_1}{z_2}^2\}$. \\

% From this we can see that the inner product in $\mathbb{R}^2$ is in fact the linear combination of 

% \section{Reproducing Kernel Hilbert Spaces}





